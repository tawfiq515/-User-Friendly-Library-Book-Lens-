{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e62d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, time, math, json, sqlite3, requests\n",
    "from itertools import product\n",
    "from typing import List, Tuple\n",
    "\n",
    "API_URL   = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "API_KEY   = None  # Ø¶ÙŠÙÙŠ Ù…ÙØªØ§Ø­Ùƒ Ù‡ÙˆÙ† Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ (Ù…Ø«Ù„Ø§Ù‹ \"AIza...\") Ø£Ùˆ Ø®Ù„ÙŠÙ‡ None\n",
    "PER_PAGE  = 40\n",
    "PER_QUERY_CAP = 1000\n",
    "SLEEP_BASE = 0.25\n",
    "\n",
    "# ğŸ“Œ Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ÙŠØªØ³ÙŠÙ Ø¨Ø§Ù„ÙÙˆÙ„Ø¯Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨)\n",
    "DB_PATH = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "TABLE_STATE = \"crawl_state\"\n",
    "\n",
    "# --------- 1) ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨Ø§ÙƒØªØ§Øª ----------\n",
    "letters = [chr(c) for c in range(ord('a'), ord('z')+1)]\n",
    "digits  = [str(d) for d in range(10)]\n",
    "author_prefixes = letters + digits   # 36 Ø¨Ø§Ø¯Ø¦Ø©\n",
    "title_prefixes  = letters            # 26 Ø¨Ø§Ø¯Ø¦Ø©\n",
    "\n",
    "def generate_buckets() -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    ÙƒÙ„ Ø¨Ø§ÙƒØª Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† (q_string, bucket_key, hint)\n",
    "    \"\"\"\n",
    "    buckets = []\n",
    "\n",
    "    # Ø§Ù„Ø¨Ø§ÙƒØªØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø­Ø±Ù ÙˆØ§Ø­Ø¯ Ù„Ù„Ø¹Ù†ÙˆØ§Ù† Ã— Ø­Ø±Ù/Ø±Ù‚Ù… Ù„Ù„Ù…Ø¤Ù„Ù)\n",
    "    for t, a in product(title_prefixes, author_prefixes):\n",
    "        q = f'intitle:{t} inauthor:{a}'\n",
    "        key = f'intitle_{t}__inauthor_{a}'\n",
    "        buckets.append((q, key, f\"title starts {t}, author starts {a}\"))\n",
    "\n",
    "    # âœ… Ø«Ù†Ø§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (aa, ab, ..., zz)\n",
    "    for t2 in (f\"{x}{y}\" for x,y in product(letters, letters)):\n",
    "        q = f'intitle:{t2}'\n",
    "        key = f'intitle_{t2}'\n",
    "        buckets.append((q, key, f\"title starts {t2}\"))\n",
    "\n",
    "    return buckets\n",
    "\n",
    "# --------- 2) Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------\n",
    "def init_db():\n",
    "    con = sqlite3.connect(DB_PATH)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_BOOKS} (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        title TEXT,\n",
    "        authors TEXT,\n",
    "        publishedDate TEXT,\n",
    "        industryIds TEXT,\n",
    "        raw JSON\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_STATE} (\n",
    "        bucket_key TEXT PRIMARY KEY,\n",
    "        q TEXT NOT NULL,\n",
    "        next_start INTEGER NOT NULL DEFAULT 0,\n",
    "        done INTEGER NOT NULL DEFAULT 0,\n",
    "        total_reported INTEGER\n",
    "    );\n",
    "    \"\"\")\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "def save_state(bucket_key, q, next_start, done, total_reported=None):\n",
    "    con = sqlite3.connect(DB_PATH)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "    INSERT INTO {TABLE_STATE} (bucket_key, q, next_start, done, total_reported)\n",
    "    VALUES (?, ?, ?, ?, ?)\n",
    "    ON CONFLICT(bucket_key)\n",
    "    DO UPDATE SET next_start=excluded.next_start, done=excluded.done,\n",
    "                  total_reported=COALESCE(excluded.total_reported, {TABLE_STATE}.total_reported);\n",
    "    \"\"\", (bucket_key, q, next_start, done, total_reported))\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "def load_state(bucket_key):\n",
    "    con = sqlite3.connect(DB_PATH)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"SELECT q, next_start, done, total_reported FROM {TABLE_STATE} WHERE bucket_key=?;\", (bucket_key,))\n",
    "    row = cur.fetchone()\n",
    "    con.close()\n",
    "    if row:\n",
    "        return {\"q\": row[0], \"next_start\": row[1], \"done\": row[2], \"total_reported\": row[3]}\n",
    "    return None\n",
    "\n",
    "def insert_books(items):\n",
    "    if not items:\n",
    "        return 0\n",
    "    con = sqlite3.connect(DB_PATH)\n",
    "    cur = con.cursor()\n",
    "    count = 0\n",
    "    for it in items:\n",
    "        bid = it.get(\"id\")\n",
    "        if not bid:\n",
    "            continue\n",
    "        vol = it.get(\"volumeInfo\", {})\n",
    "        title = vol.get(\"title\")\n",
    "        authors = \", \".join(vol.get(\"authors\", [])) if vol.get(\"authors\") else None\n",
    "        pubdate = vol.get(\"publishedDate\")\n",
    "        inds = vol.get(\"industryIdentifiers\", [])\n",
    "        inds_str = json.dumps(inds, ensure_ascii=False)\n",
    "        raw = json.dumps(it, ensure_ascii=False)\n",
    "        try:\n",
    "            cur.execute(f\"\"\"\n",
    "            INSERT OR IGNORE INTO {TABLE_BOOKS}(id, title, authors, publishedDate, industryIds, raw)\n",
    "            VALUES (?, ?, ?, ?, ?, ?);\n",
    "            \"\"\", (bid, title, authors, pubdate, inds_str, raw))\n",
    "            count += cur.rowcount\n",
    "        except sqlite3.Error:\n",
    "            pass\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return count\n",
    "\n",
    "# --------- 3) Ø·Ù„Ø¨ API Ù…Ø¹ Backoff ----------\n",
    "def get_page(q, start_index=0, per_page=PER_PAGE):\n",
    "    params = {\"q\": q, \"maxResults\": per_page, \"startIndex\": start_index}\n",
    "    if API_KEY:\n",
    "        params[\"key\"] = API_KEY\n",
    "\n",
    "    backoff = SLEEP_BASE\n",
    "    for attempt in range(6):\n",
    "        r = requests.get(API_URL, params=params, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        elif r.status_code in (429, 403, 500, 503):\n",
    "            time.sleep(backoff)\n",
    "            backoff = min(backoff * 2, 8.0)\n",
    "        else:\n",
    "            raise requests.HTTPError(f\"{r.status_code} {r.reason} â€” {r.url}\")\n",
    "    r.raise_for_status()\n",
    "\n",
    "# --------- 4) Ø²Ø­Ù Ø¨Ø§ÙƒØª ÙˆØ§Ø­Ø¯ ----------\n",
    "def crawl_bucket(q, bucket_key):\n",
    "    st = load_state(bucket_key)\n",
    "    start = 0\n",
    "    total_reported = None\n",
    "\n",
    "    if st:\n",
    "        if st[\"done\"]:\n",
    "            return 0, 0, st.get(\"total_reported\")\n",
    "        start = st[\"next_start\"] or 0\n",
    "        total_reported = st.get(\"total_reported\")\n",
    "\n",
    "    if start == 0:\n",
    "        first = get_page(q, start_index=0)\n",
    "        total_reported = first.get(\"totalItems\", 0)\n",
    "        items = first.get(\"items\", []) or []\n",
    "        insert_books(items)\n",
    "        start = len(items)\n",
    "        save_state(bucket_key, q, start, 0, total_reported)\n",
    "        time.sleep(SLEEP_BASE)\n",
    "\n",
    "    inserted_total = 0\n",
    "    while start < min(total_reported or PER_QUERY_CAP, PER_QUERY_CAP):\n",
    "        try:\n",
    "            data = get_page(q, start_index=start)\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"[{bucket_key}] HTTPError at start={start}: {e}\")\n",
    "            break\n",
    "        items = data.get(\"items\", []) or []\n",
    "        if not items:\n",
    "            break\n",
    "        inserted = insert_books(items)\n",
    "        inserted_total += inserted\n",
    "        start += len(items)\n",
    "        save_state(bucket_key, q, start, 0, total_reported)\n",
    "        time.sleep(SLEEP_BASE)\n",
    "\n",
    "    save_state(bucket_key, q, start, 1, total_reported)\n",
    "    return inserted_total, start, total_reported\n",
    "\n",
    "# --------- 5) Ø²Ø­Ù Ø§Ù„ÙƒÙ„ ----------\n",
    "def crawl_all(target_count=1_000_000):\n",
    "    init_db()\n",
    "    buckets = generate_buckets()\n",
    "    total_before = count_books()\n",
    "    print(f\"Existing in DB before: {total_before}\")\n",
    "\n",
    "    for i, (q, key, hint) in enumerate(buckets, 1):\n",
    "        st = load_state(key)\n",
    "        if st and st[\"done\"]:\n",
    "            continue\n",
    "        print(f\"[{i}/{len(buckets)}] Bucket: {key} â€” {hint}\")\n",
    "        inserted, fetched, total_reported = crawl_bucket(q, key)\n",
    "        current = count_books()\n",
    "        print(f\"  inserted:{inserted}  fetchedâ‰ˆ{fetched}  total_reported={total_reported}  DB now:{current}\")\n",
    "        if current >= target_count:\n",
    "            print(\"Reached target. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Final DB count: {count_books()}\")\n",
    "\n",
    "def count_books():\n",
    "    con = sqlite3.connect(DB_PATH)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {TABLE_BOOKS};\")\n",
    "    c = cur.fetchone()[0]\n",
    "    con.close()\n",
    "    return c\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_all(target_count=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "\n",
    "# Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©\n",
    "con = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Ø¬Ù„Ø¨ Ø£ÙˆÙ„ ØµÙ ÙÙ‚Ø· Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS} LIMIT 1\", con)\n",
    "con.close()\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ¹Ø¯Ø¯Ù‡Ø§\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Number of columns:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f941d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "\n",
    "# Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©\n",
    "con = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS}\", con)\n",
    "con.close()\n",
    "\n",
    "# Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ (Ø§Ù„Ø±ÙŠÙƒÙˆØ±Ø¯)\n",
    "total_records = len(df)\n",
    "print(f\"Total records in DB: {total_records}\\n\")\n",
    "\n",
    "# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
    "for col, nulls in null_counts.items():\n",
    "    print(f\"{col}: {nulls} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "\n",
    "# Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©\n",
    "con = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS}\", con)\n",
    "con.close()\n",
    "\n",
    "# Ø·Ø¨Ø§Ø¹Ø© Ø£ÙˆÙ„ 5 ØµÙÙˆÙ Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯\n",
    "for col in df.columns:\n",
    "    print(f\"--- Column: {col} ---\")\n",
    "    print(df[col].head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "db_path = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\books_crawl.sqlite\"\n",
    "new_db = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\new_books_crawl.sqlite\"\n",
    "\n",
    "shutil.copyfile(db_path, new_db)\n",
    "print(\"Copy created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f48b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Ù…Ø³Ø§Ø± Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\n",
    "new_db = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\new_books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "\n",
    "# ÙØªØ­ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³\n",
    "con = sqlite3.connect(new_db)\n",
    "\n",
    "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„\n",
    "try:\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS}\", con)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    con.close()\n",
    "    raise\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ (Ø§Ù„Ø±ÙŠÙƒÙˆØ±Ø¯)\n",
    "total_records = len(df)\n",
    "print(f\"Total records in DB: {total_records}\\n\")\n",
    "\n",
    "# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
    "for col, nulls in null_counts.items():\n",
    "    print(f\"{col}: {nulls} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Ù…Ø³Ø§Ø± Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\n",
    "new_db = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\new_books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "\n",
    "# ÙØªØ­ Ø§Ù„Ø§ØªØµØ§Ù„\n",
    "con = sqlite3.connect(new_db)\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS}\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97650c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù†Ø¹Ù…Ù„ Ø£Ø¹Ù…Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† industryIds\n",
    "def extract_industry_ids(industry_json):\n",
    "    try:\n",
    "        data = json.loads(industry_json)\n",
    "        # Ù†Ø¬Ù…Ø¹ ÙƒÙ„ identifiers Ø¨ÙØ§ØµÙ„Ø©\n",
    "        return ', '.join(item.get(\"identifier\", \"\") for item in data)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['industry_identifiers'] = df['industryIds'].apply(extract_industry_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfe36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(raw_json):\n",
    "    try:\n",
    "        data = json.loads(raw_json)\n",
    "        volume_info = data.get(\"volumeInfo\", {})\n",
    "        sale_info = data.get(\"saleInfo\", {})\n",
    "        access_info = data.get(\"accessInfo\", {})\n",
    "\n",
    "        return pd.Series({\n",
    "            \"selfLink\": data.get(\"selfLink\"),\n",
    "            \"title\": volume_info.get(\"title\"),\n",
    "            \"authors\": ', '.join(volume_info.get(\"authors\", [])) if volume_info.get(\"authors\") else None,\n",
    "            \"publishedDate\": volume_info.get(\"publishedDate\"),\n",
    "            \"description\": volume_info.get(\"description\"),\n",
    "            \"reading_text\": volume_info.get(\"readingModes\", {}).get(\"text\"),\n",
    "            \"reading_image\": volume_info.get(\"readingModes\", {}).get(\"image\"),\n",
    "            \"pageCount\": volume_info.get(\"pageCount\"),\n",
    "            \"printType\": volume_info.get(\"printType\"),\n",
    "            \"categories\": ', '.join(volume_info.get(\"categories\", [])) if volume_info.get(\"categories\") else None,\n",
    "            \"thumbnail\": volume_info.get(\"imageLinks\", {}).get(\"thumbnail\"),\n",
    "            \"language\": volume_info.get(\"language\"),\n",
    "            \"infoLink\": volume_info.get(\"infoLink\"),\n",
    "            \"saleability\": sale_info.get(\"saleability\"),\n",
    "            \"isEbook\": sale_info.get(\"isEbook\"),\n",
    "            \"pdf_available\": access_info.get(\"pdf\", {}).get(\"isAvailable\")\n",
    "        })\n",
    "    \n",
    "    except:\n",
    "        return pd.Series([None]*17, index=[\n",
    "            \"selfLink\",\"title\",\"authors\",\"publishedDate\",\"description\",\n",
    "            \"reading_text\",\"reading_image\",\"pageCount\",\"printType\",\n",
    "            \"categories\",\"thumbnail\",\"language\",\"infoLink\",\"saleability\",\n",
    "            \"isEbook\",\"pdf_available\"\n",
    "        ])\n",
    "\n",
    "# Ù†ÙÙƒ ÙƒÙ„ Ø§Ù„ÙÙŠØªØ´Ø±Ø§Øª Ù…Ù† raw\n",
    "features_df = df['raw'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df, features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70791525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.columns[df_final.columns.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a257e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.loc[:, ~df_final.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeeebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(new_db)\n",
    "df_final.to_sql(TABLE_BOOKS, con, if_exists=\"replace\", index=False)\n",
    "con.close()\n",
    "print(\"Features extracted and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7cdc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "con = sqlite3.connect(new_db)\n",
    "df_check = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS}\", con)\n",
    "con.close()\n",
    "\n",
    "print(f\"Total records in new DB: {len(df_check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_check.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_check.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc81a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ…\n",
    "num_columns = len(df_final.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
    "for col, nulls in null_counts.items():\n",
    "    print(f\"{col}: {nulls} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Ù†Ø­Ø°Ù Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠÙ† Ù…Ù† Ø§Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ…\n",
    "df_final = df_final.drop(columns=['raw', 'industryIds'])\n",
    "\n",
    "# Ù†ÙƒØªØ¨ Ø§Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ… Ø§Ù„Ù…Ø­Ø¯Ø«Ø© Ø¨Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³\n",
    "con = sqlite3.connect(new_db)  # Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³ Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
    "df_final.to_sql(TABLE_BOOKS, con, if_exists=\"replace\", index=False)\n",
    "con.close()\n",
    "\n",
    "print(\"raw Ùˆ industryIds ØªÙ… Ø­Ø°ÙÙ‡Ù… ÙˆØ§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³ ØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ø¨Ù†Ø¬Ø§Ø­!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ…\n",
    "num_columns = len(df_final.columns)\n",
    "print(f\"Number of columns: {num_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
    "for col, nulls in null_counts.items():\n",
    "    print(f\"{col}: {nulls} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22562c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÙØªØ­ Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³ Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "con = sqlite3.connect(new_db)\n",
    "df_sample = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS} LIMIT 5\", con)\n",
    "con.close()\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø§Ù„Ø¹ÙŠÙ†Ø©\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf314a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„ÙƒØªØ¨ Ø¨ÙƒÙ„ ØªØµÙ†ÙŠÙ\n",
    "category_counts = df_final['categories'].value_counts(dropna=False)\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce5de2",
   "metadata": {},
   "source": [
    "\n",
    "# cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93658984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "new_db = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\new_books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "\n",
    "# Ø§Ù„Ø§ØªØµØ§Ù„\n",
    "con = sqlite3.connect(new_db)\n",
    "\n",
    "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS}\", con)\n",
    "\n",
    "print(\"Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù:\", len(df))\n",
    "\n",
    "# Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ null Ø¨Ù€ title Ø£Ùˆ authors\n",
    "df = df.dropna(subset=[\"title\", \"authors\"])\n",
    "\n",
    "print(\"Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù:\", len(df))\n",
    "\n",
    "# Ø­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³\n",
    "df.to_sql(TABLE_BOOKS, con, if_exists=\"replace\", index=False)\n",
    "con.close()\n",
    "\n",
    "print(\"ØªÙ… Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ null Ø¨Ù€ title Ø£Ùˆ authors ÙˆØ­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ³\n",
    "new_db = r\"C:\\Users\\Rama Al Jada\\Desktop\\testbook\\new_books_crawl.sqlite\"\n",
    "TABLE_BOOKS = \"books\"\n",
    "\n",
    "con = sqlite3.connect(new_db)\n",
    "\n",
    "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¯Ø§ØªØ§\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {TABLE_BOOKS}\", con)\n",
    "\n",
    "# Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ null Ø¨Ø§Ù„Ù€ title Ø£Ùˆ authors\n",
    "df = df.dropna(subset=[\"title\", \"authors\"])\n",
    "\n",
    "# Ù†Ø´ÙˆÙ nulls Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:\\n\")\n",
    "for col, nulls in null_counts.items():\n",
    "    print(f\"{col}: {nulls} null values\")\n",
    "\n",
    "# Ù†Ø±Ø¬Ø¹ Ù†Ø®Ø²Ù† Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ù…Ù†Ø¸ÙØ© Ù…ÙƒØ§Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„\n",
    "df.to_sql(TABLE_BOOKS, con, if_exists=\"replace\", index=False)\n",
    "\n",
    "con.close()\n",
    "print(\"\\nØªÙ… Ø§Ù„Ø­Ø°Ù ÙˆØ§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­ âœ…\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
